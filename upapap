#!/usr/bin/env python3
# encoding: utf-8 (as per PEP 263)

import sys

import requests
from bs4 import BeautifulSoup

urls = [
    'https://www.fim.uni-passau.de/studium/modulkataloge/',
    'https://www.fim.uni-passau.de/index.php?id=17010',
    'https://www.fim.uni-passau.de/ueber-die-fakultaet/ausschuesse/pruefungsausschuss/',
    'https://www.fim.uni-passau.de/studium/pruefungsordnungen/',
]

def parse_lolol(soup):
    """Parse list of lists of links. Requires full page soup as input."""
    
    pagetitle = soup.find('header', {'class': 'h1'}).find('h1').get_text()
    print('# %s' % pagetitle)
    
    content = soup.find('main')

    last_heading = ""
    for l in content.find_all('ul'):
        heading = l.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']).get_text()
        if heading != last_heading:
        	print()
        	print('## %s' % heading)
        	last_heading = heading
        for i in l.find_all('li'):
            text = i.get_text()
            a = i.find('a')
            if not a:
                print('* %s  ' % (text))
            else:
                href = i.find('a').attrs['href']
                if not href.startswith('http://') and not href.startswith('https://'):
                    if not href.startswith('/'):
                        href = '/' + href
                    href = 'https://www.fim.uni-passau.de' + href
                print('* %s  \n  %s' % (text, href))
    print()

def main():
    for url in urls:
        req = requests.get(url)
        soup = BeautifulSoup(req.content, 'lxml')
        parse_lolol(soup)

if __name__ == '__main__':
    main()
